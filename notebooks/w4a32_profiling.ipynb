{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfAF9EqNtMkG",
        "outputId": "c1f193c8-9e40-4e03-917c-c954d69914d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "039cOClfu9F2",
        "outputId": "ee5e990b-026b-43a4-fa0a-ece894206bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing Nsight Systems...\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.0 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,361 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:12 https://cli.github.com/packages stable/main amd64 Packages [355 B]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,904 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,070 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,756 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,301 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,614 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,749 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,734 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,529 kB]\n",
            "Fetched 39.6 MB in 7s (5,984 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n",
            "  libxkbcommon-x11-0 libxtst6\n",
            "The following NEW packages will be installed:\n",
            "  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n",
            "  libxkbcommon-x11-0 libxtst6 nsight-systems-2023.3.3\n",
            "0 upgraded, 13 newly installed, 0 to remove and 63 not upgraded.\n",
            "Need to get 325 MB of archives.\n",
            "After this operation, 1,302 kB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.3.3 2023.3.3.42-233333266658v0 [324 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Fetched 325 MB in 4s (73.6 MB/s)\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "(Reading database ... 121852 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../01-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../02-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../03-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../04-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../05-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../07-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../08-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../09-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../10-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../11-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package nsight-systems-2023.3.3.\n",
            "Preparing to unpack .../12-nsight-systems-2023.3.3_2023.3.3.42-233333266658v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2023.3.3 (2023.3.3.42-233333266658v0) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up nsight-systems-2023.3.3 (2023.3.3.42-233333266658v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.3.3/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.3.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Found ncu in /usr/local/cuda/bin.\n",
            "\n",
            "=== Installation Check ===\n",
            "NVIDIA Nsight Systems version 2023.3.3.42-233333266658v0\n",
            "NVIDIA (R) Nsight Compute Command Line Profiler\n",
            "Copyright (c) 2018-2025 NVIDIA Corporation\n",
            "Version 2025.1.1.0 (build 35528883) (public-release)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. Nsight Systems (nsys) 설치 여부 확인 및 경로 설정\n",
        "if os.path.exists('/usr/local/cuda/bin/nsys'):\n",
        "    print(\"Found nsys in /usr/local/cuda/bin. Adding to PATH...\")\n",
        "    os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "else:\n",
        "    print(\"Installing Nsight Systems...\")\n",
        "    # 최신 버전 설치 (apt repository 업데이트)\n",
        "    !apt-get update -y\n",
        "    !apt-get install -y nsight-systems-2023.3.3\n",
        "    # 설치 후 경로 추가 (보통 /usr/local/bin에 생기지만 혹시 모르니)\n",
        "    os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "\n",
        "# 2. Nsight Compute (ncu) 설치 여부 확인\n",
        "if os.path.exists('/usr/local/cuda/bin/ncu'):\n",
        "    print(\"Found ncu in /usr/local/cuda/bin.\")\n",
        "else:\n",
        "    print(\"Installing Nsight Compute...\")\n",
        "    !apt-get install -y nsight-compute-2023.3.1\n",
        "    os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "\n",
        "print(\"\\n=== Installation Check ===\")\n",
        "!nsys --version\n",
        "!ncu --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ki4V9VrqNIK",
        "outputId": "3b048c65-6855-447c-cbc8-321e9aa3d3d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing profile_run.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile profile_run.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "import torch.cuda.nvtx as nvtx\n",
        "import time\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. CUDA Kernel Source (작성하신 코드 그대로)\n",
        "# ---------------------------------------------------------\n",
        "cuda_source = \"\"\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <torch/extension.h>\n",
        "\n",
        "template<int BM, int BN, int BK, int TM, int TN_PHY>\n",
        "__global__ void sgemm2D_kernel(int M, int K, int N, float* A, int* B, float* C, float* scale, float* zero_point)\n",
        "{\n",
        "    int cRow = blockIdx.y;\n",
        "    int cCol = blockIdx.x;\n",
        "\n",
        "    constexpr int TN_LOG = TN_PHY * 8;\n",
        "\n",
        "    int threadRow = threadIdx.x / (BN / TN_LOG);\n",
        "    int threadCol = threadIdx.x % (BN / TN_LOG);\n",
        "\n",
        "    __shared__ float As[BM * BK];\n",
        "    __shared__ int Bs[BK * (BN / 8)];\n",
        "\n",
        "    // 128 * 8 = 1024 , 256 threads ,\n",
        "    int innerRowA = threadIdx.x / (BK / 4);\n",
        "    int innerColA = threadIdx.x % (BK / 4);\n",
        "\n",
        "    // (128 / 8) * 8 = 128\n",
        "    int innerRowB = threadIdx.x / (BN / 8 / 4);\n",
        "    int innerColB = threadIdx.x % (BN / 8 / 4);\n",
        "\n",
        "    A += cRow * BM * K;\n",
        "    B += cCol * (BN / 8);\n",
        "    C += cRow * BM * N + cCol * BN;\n",
        "\n",
        "    float threadResults[TM * TN_LOG] = {0.0};\n",
        "    float regM[TM] = {0.0};\n",
        "    int regN[TN_PHY] = {0};\n",
        "\n",
        "    float my_scales[TN_LOG];\n",
        "    float my_zeros[TN_LOG];\n",
        "\n",
        "    for (int i = 0; i < TN_LOG; ++i) {\n",
        "        int globalN = cCol * BN + threadCol * TN_LOG + i;\n",
        "        if (globalN < N) {\n",
        "            my_scales[i] = scale[globalN];\n",
        "            my_zeros[i]  = zero_point[globalN];\n",
        "        }\n",
        "        else {\n",
        "            my_scales[i] = 1.0f;\n",
        "            my_zeros[i] = 0.0f;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int bk = 0; bk < K; bk += BK) {\n",
        "\n",
        "        int globalM = cRow * BM + innerRowA;\n",
        "        int globalK = bk + innerColA * 4;\n",
        "\n",
        "        float4 tmp = {0.0f, 0.0f, 0.0f, 0.0f};\n",
        "\n",
        "        if (globalM < M) {\n",
        "            if (globalK + 4 <= K) {\n",
        "                tmp = reinterpret_cast<float4 *>(&A[innerRowA * K + innerColA * 4])[0];\n",
        "            }\n",
        "            else {\n",
        "                float* ptr = &A[innerRowA * K + innerColA * 4];\n",
        "                if (globalK + 0 < K) tmp.x = ptr[0];\n",
        "                if (globalK + 1 < K) tmp.y = ptr[1];\n",
        "                if (globalK + 2 < K) tmp.z = ptr[2];\n",
        "                if (globalK + 3 < K) tmp.w = ptr[3];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        As[(innerColA * 4 + 0) * BM + innerRowA] = tmp.x;\n",
        "        As[(innerColA * 4 + 1) * BM + innerRowA] = tmp.y;\n",
        "        As[(innerColA * 4 + 2) * BM + innerRowA] = tmp.z;\n",
        "        As[(innerColA * 4 + 3) * BM + innerRowA] = tmp.w;\n",
        "\n",
        "        // 8 * 4 = 32 개 불러옴\n",
        "        if (threadIdx.x < BK * (BN / 8) / 4) {\n",
        "\n",
        "            if (bk + innerRowB < K) {\n",
        "                reinterpret_cast<int4 *>(&Bs[innerRowB * (BN / 8) + innerColB * 4])[0] =\n",
        "                reinterpret_cast<int4 *>(&B[innerRowB * (N / 8) + innerColB * 4])[0];\n",
        "            }\n",
        "            else {\n",
        "                reinterpret_cast<int4 *>(&Bs[innerRowB * (BN / 8) + innerColB * 4])[0] = {0, 0, 0, 0};\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        A += BK;\n",
        "        B += BK * (N / 8);\n",
        "\n",
        "        for (int dot = 0; dot < BK; ++dot) {\n",
        "            for (int i = 0; i < TM; ++i){\n",
        "                regM[i] = As[dot * BM + threadRow * TM + i];\n",
        "            }\n",
        "            for (int i = 0; i < TN_PHY; ++i) {\n",
        "                regN[i] = Bs[dot * (BN / 8) + threadCol * TN_PHY + i];\n",
        "            }\n",
        "\n",
        "            for (int i = 0; i < TN_PHY; ++i) {\n",
        "                int packed_val = regN[i];\n",
        "\n",
        "                for (int subN = 0; subN < 8; ++subN) {\n",
        "                    int int4_val = (packed_val >> (subN * 4)) & 0xF;\n",
        "                    float real_val = (float(int4_val) - my_zeros[i * 8 + subN]) * my_scales[i * 8 + subN];\n",
        "\n",
        "                    for (int m = 0; m < TM; ++m) {\n",
        "                        int resNidx = i * 8 + subN;\n",
        "\n",
        "                        threadResults[m * TN_LOG + resNidx] += regM[m] * real_val;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    for (uint resIdxM = 0; resIdxM < TM; resIdxM += 1) {\n",
        "        for (uint resIdxN = 0; resIdxN < TN_LOG; resIdxN += 4) {\n",
        "            int globalRowC = cRow * BM + threadRow * TM + resIdxM;\n",
        "            int globalColC = cCol * BN + threadCol * TN_LOG + resIdxN;\n",
        "\n",
        "            if (globalRowC < M) {\n",
        "                float4 tmp;\n",
        "                tmp.x = threadResults[resIdxM * TN_LOG + resIdxN];\n",
        "                tmp.y = threadResults[resIdxM * TN_LOG + resIdxN + 1];\n",
        "                tmp.z = threadResults[resIdxM * TN_LOG + resIdxN + 2];\n",
        "                tmp.w = threadResults[resIdxM * TN_LOG + resIdxN + 3];\n",
        "\n",
        "                if (globalColC + 4 <= N) {\n",
        "                    reinterpret_cast<float4 *>(&C[(threadRow * TM + resIdxM) * N + threadCol * TN_LOG + resIdxN])[0] = tmp;\n",
        "                }\n",
        "                else {\n",
        "                    float* ptr = &C[(threadRow * TM + resIdxM) * N + threadCol * TN_LOG + resIdxN];\n",
        "                    if (globalColC + 0 < N) ptr[0] = tmp.x;\n",
        "                    if (globalColC + 1 < N) ptr[1] = tmp.y;\n",
        "                    if (globalColC + 2 < N) ptr[2] = tmp.z;\n",
        "                    if (globalColC + 3 < N) ptr[3] = tmp.w;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void sgemm_int4_cuda(torch::Tensor A, torch::Tensor B_packed, torch::Tensor C, torch::Tensor scale, torch::Tensor zero_point) {\n",
        "\n",
        "    // [Safety Checks]\n",
        "    // 커널이 Vectorized Load를 수행하므로 차원과 정렬이 안 맞으면 SegFault 발생.\n",
        "    // 따라서 C++ 단에서 엄격하게 검사합니다.\n",
        "\n",
        "    int M = A.size(0);\n",
        "    int K = A.size(1);\n",
        "    int N = C.size(1);\n",
        "\n",
        "    const int BM = 128;\n",
        "    const int BN = 128;\n",
        "    const int BK = 8;\n",
        "\n",
        "    // 1. 배수 조건 검사\n",
        "    TORCH_CHECK(N % BN == 0, \"N must be a multiple of 128\");\n",
        "\n",
        "    // 2. Vectorization 조건 검사\n",
        "    // A inner dim (K) must be multiple of 4 (float4)\n",
        "    TORCH_CHECK(K % 4 == 0, \"K must be multiple of 4 for float4 loading\");\n",
        "    // B packed inner dim (N/8) must be multiple of 4 (int4 loading) -> N multiple of 32\n",
        "    TORCH_CHECK(N % 32 == 0, \"N must be multiple of 32 for int4 loading\");\n",
        "\n",
        "    // 3. Contiguity 검사\n",
        "    TORCH_CHECK(A.is_contiguous(), \"A must be contiguous\");\n",
        "    TORCH_CHECK(B_packed.is_contiguous(), \"B_packed must be contiguous\");\n",
        "    TORCH_CHECK(C.is_contiguous(), \"C must be contiguous\");\n",
        "    TORCH_CHECK(scale.is_contiguous(), \"scale must be contiguous\");\n",
        "    TORCH_CHECK(zero_point.is_contiguous(), \"zero_point must be contiguous\");\n",
        "\n",
        "    dim3 blockDim(256);\n",
        "    dim3 gridDim((N + BN - 1) / BN, (M + BM - 1) / BM);\n",
        "\n",
        "    sgemm2D_kernel<BM, BN, BK, 8, 1><<<gridDim, blockDim>>>(\n",
        "        M, K, N,\n",
        "        A.data_ptr<float>(),\n",
        "        B_packed.data_ptr<int>(),\n",
        "        C.data_ptr<float>(),\n",
        "        scale.data_ptr<float>(),\n",
        "        zero_point.data_ptr<float>()\n",
        "    );\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "cpp_source = \"void sgemm_int4_cuda(torch::Tensor A, torch::Tensor B_packed, torch::Tensor C, torch::Tensor scale, torch::Tensor zero_point);\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Compile (JIT)\n",
        "# ---------------------------------------------------------\n",
        "sgemm_module = load_inline(\n",
        "    name=\"sgemm_int4_v1\",\n",
        "    cpp_sources=cpp_source,\n",
        "    cuda_sources=cuda_source, # 주의: 실제로는 위에서 정의한 긴 문자열이 들어가야 합니다.\n",
        "    functions=['sgemm_int4_cuda'],\n",
        "    verbose=False,\n",
        "    with_cuda=True,\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"]\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Quantized Layer Definition\n",
        "# ---------------------------------------------------------\n",
        "class QuantizedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, BM=128, BK=8, BN=128):\n",
        "        super().__init__()\n",
        "        # ... (작성하신 클래스 초기화 코드) ...\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.K = self.in_channels * self.kernel_size * self.kernel_size\n",
        "        self.N = self.out_channels\n",
        "\n",
        "        # Buffer 등록 (더미 데이터로 초기화)\n",
        "        pad_n = (BN - self.N % BN) % BN\n",
        "        pad_k = (BK - self.K % BK) % BK\n",
        "        self.register_buffer('w_packed', torch.zeros(self.K + pad_k, (self.N + pad_n) // 8, dtype=torch.int32).cuda())\n",
        "        self.register_buffer('scale', torch.ones(self.N + pad_n, dtype=torch.float32).cuda())\n",
        "        self.register_buffer('zero_point', torch.zeros(self.N + pad_n, dtype=torch.float32).cuda())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ... (작성하신 forward 코드) ...\n",
        "        # 간단한 테스트를 위해 핵심 로직만 재구성\n",
        "        batch_size, _, h_in, w_in = x.shape\n",
        "        x_unfold = F.unfold(x, kernel_size=self.kernel_size, padding=self.padding, stride=self.stride)\n",
        "        input = x_unfold.transpose(1, 2).reshape(-1, self.K).contiguous()\n",
        "\n",
        "        M, K = input.shape\n",
        "        N_padded = self.w_packed.shape[1] * 8\n",
        "        C_padded = torch.empty((M, N_padded), dtype=torch.float32, device=x.device)\n",
        "\n",
        "        # ★★★ 여기가 프로파일링 대상입니다 ★★★\n",
        "        sgemm_module.sgemm_int4_cuda(input, self.w_packed, C_padded, self.scale, self.zero_point)\n",
        "\n",
        "        return C_padded # 뒷부분 생략\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Main Execution Block for Profiling\n",
        "# ---------------------------------------------------------\n",
        "def main():\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    # 모델 생성 (ResNet 전체 대신 무거운 레이어 하나만 테스트해도 충분합니다)\n",
        "    # 실제로는 전체 모델을 로드해서 하셔도 됩니다.\n",
        "    model = QuantizedConv2d(64, 128, kernel_size=3, stride=1, padding=1).to(device)\n",
        "    dummy_input = torch.randn(32, 64, 32, 32).to(device)\n",
        "\n",
        "    # Warm-up\n",
        "    print(\"Warm-up...\")\n",
        "    for _ in range(5):\n",
        "        model(dummy_input)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Profiling Run\n",
        "    print(\"Profiling Start...\")\n",
        "\n",
        "    # NVTX Range로 전체 구간 표시\n",
        "    nvtx.range_push(\"My_Model_Inference\")\n",
        "\n",
        "    for i in range(10): # 10번 반복\n",
        "        nvtx.range_push(f\"Iter_{i}\")\n",
        "        model(dummy_input)\n",
        "        torch.cuda.synchronize() # 정확한 시간 측정을 위해 (배포시는 제거)\n",
        "        nvtx.range_pop()\n",
        "\n",
        "    nvtx.range_pop()\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uNzNAWgAhdz",
        "outputId": "da382e02-97cb-469a-8665-6ffe156e43c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warm-up...\n",
            "Profiling Start...\n",
            "Done.\n",
            "Generating '/tmp/nsys-report-a59d.qdstrm'\n",
            "[1/7] [========================100%] nsys_result_32_im2col_fuse.nsys-rep\n",
            "[2/7] [========================100%] nsys_result_32_im2col_fuse.sqlite\n",
            "[3/7] Executing 'nvtx_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)   Style         Range       \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  -------  ------------------\n",
            "     50.0       51,499,099          1  51,499,099.0  51,499,099.0  51,499,099  51,499,099          0.0  PushPop  My_Model_Inference\n",
            "      5.1        5,204,104          1   5,204,104.0   5,204,104.0   5,204,104   5,204,104          0.0  PushPop  Iter_0            \n",
            "      5.0        5,159,819          1   5,159,819.0   5,159,819.0   5,159,819   5,159,819          0.0  PushPop  Iter_1            \n",
            "      5.0        5,146,267          1   5,146,267.0   5,146,267.0   5,146,267   5,146,267          0.0  PushPop  Iter_5            \n",
            "      5.0        5,142,368          1   5,142,368.0   5,142,368.0   5,142,368   5,142,368          0.0  PushPop  Iter_4            \n",
            "      5.0        5,141,772          1   5,141,772.0   5,141,772.0   5,141,772   5,141,772          0.0  PushPop  Iter_2            \n",
            "      5.0        5,141,021          1   5,141,021.0   5,141,021.0   5,141,021   5,141,021          0.0  PushPop  Iter_9            \n",
            "      5.0        5,139,630          1   5,139,630.0   5,139,630.0   5,139,630   5,139,630          0.0  PushPop  Iter_3            \n",
            "      5.0        5,123,450          1   5,123,450.0   5,123,450.0   5,123,450   5,123,450          0.0  PushPop  Iter_8            \n",
            "      5.0        5,116,329          1   5,116,329.0   5,116,329.0   5,116,329   5,116,329          0.0  PushPop  Iter_6            \n",
            "      5.0        5,114,152          1   5,114,152.0   5,114,152.0   5,114,152   5,114,152          0.0  PushPop  Iter_7            \n",
            "\n",
            "[4/7] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)                Name               \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ----------  -----------  ---------------------------------\n",
            "     89.6       75,241,292         11  6,840,117.5  5,032,341.0  4,994,242  24,964,946  6,011,338.3  cudaDeviceSynchronize            \n",
            "      3.0        2,529,005          1  2,529,005.0  2,529,005.0  2,529,005   2,529,005          0.0  cudaGetDeviceProperties_v2_v12000\n",
            "      2.6        2,156,588         15    143,772.5     14,974.0      7,206   1,936,786    496,063.9  cudaLaunchKernel                 \n",
            "      2.3        1,923,403          5    384,680.6     17,085.0      5,257   1,842,445    815,110.5  cudaMemcpyAsync                  \n",
            "      1.5        1,281,389          3    427,129.7      5,320.0      2,626   1,273,443    732,930.1  cudaStreamIsCapturing_v10000     \n",
            "      0.9          730,647          3    243,549.0    243,700.0    203,854     283,093     39,619.7  cudaMalloc                       \n",
            "      0.1          122,279          5     24,455.8      9,138.0      6,001      83,984     33,479.6  cudaStreamSynchronize            \n",
            "      0.0            1,569          1      1,569.0      1,569.0      1,569       1,569          0.0  cuModuleGetLoadingMode           \n",
            "\n",
            "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                                                  Name                                                \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------------------------------------------------------------\n",
            "    100.0       75,860,592         15  5,057,372.8  5,057,665.0  5,054,945  5,060,321      1,484.9  void sgemm2D<(int)64, (int)64, (int)8, (int)4, (int)1>(float *, int *, float *, float *, float *, i…\n",
            "\n",
            "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)  Min (ns)  Max (ns)   StdDev (ns)           Operation          \n",
            " --------  ---------------  -----  ---------  --------  --------  ---------  -----------  ----------------------------\n",
            "    100.0        1,684,885      5  336,977.0     704.0       704  1,677,365    749,302.4  [CUDA memcpy Host-to-Device]\n",
            "\n",
            "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
            "      8.427      5     1.685     0.001     0.001     8.389        3.747  [CUDA memcpy Host-to-Device]\n",
            "\n",
            "Generated:\n",
            "    /content/nsys_result_32_im2col_fuse.nsys-rep\n",
            "    /content/nsys_result_32_im2col_fuse.sqlite\n"
          ]
        }
      ],
      "source": [
        "# --trace=cuda,nvtx,osrt  <-- 여기서 osrt 제거\n",
        "!nsys profile \\\n",
        "  --trace=cuda,nvtx \\\n",
        "  --output=nsys_result_32_im2col_fuse \\\n",
        "  --force-overwrite=true \\\n",
        "  --stats=true \\\n",
        "  python profile_run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdtkqFeqApsQ",
        "outputId": "fcbf2131-874b-499c-a25f-153c7636b8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: ./sgemm_run: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!./sgemm_run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTexcp6GYKCn",
        "outputId": "a45d4f22-1a82-4ea8-e761-9dec14ba9932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==PROF== Connected to process 2318 (/usr/bin/python3.12)\n",
            "Warm-up...\n",
            "Profiling Start...\n",
            "Done.\n",
            "==PROF== Disconnected from process 2318\n",
            "==WARNING== No kernels were profiled.\n",
            "Available Kernels:\n",
            "1. sgemm2D\n"
          ]
        }
      ],
      "source": [
        "# Colab 셀에서 실행\n",
        "!ncu \\\n",
        "  --set full \\\n",
        "  --kernel-name regex:sgemm2D \\\n",
        "  --launch-count 1 \\\n",
        "  -o ncu_result_32_im2col_fuse \\\n",
        "  -f \\\n",
        "  python profile_run.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
