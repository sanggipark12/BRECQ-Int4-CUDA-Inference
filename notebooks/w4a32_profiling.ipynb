{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfAF9EqNtMkG",
        "outputId": "c1f193c8-9e40-4e03-917c-c954d69914d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "039cOClfu9F2",
        "outputId": "ee5e990b-026b-43a4-fa0a-ece894206bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing Nsight Systems...\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.0 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,361 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:12 https://cli.github.com/packages stable/main amd64 Packages [355 B]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,904 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,070 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,756 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,301 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,614 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,749 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,734 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,529 kB]\n",
            "Fetched 39.6 MB in 7s (5,984 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n",
            "  libxkbcommon-x11-0 libxtst6\n",
            "The following NEW packages will be installed:\n",
            "  libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxcomposite1\n",
            "  libxkbcommon-x11-0 libxtst6 nsight-systems-2023.3.3\n",
            "0 upgraded, 13 newly installed, 0 to remove and 63 not upgraded.\n",
            "Need to get 325 MB of archives.\n",
            "After this operation, 1,302 kB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.3.3 2023.3.3.42-233333266658v0 [324 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Fetched 325 MB in 4s (73.6 MB/s)\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "(Reading database ... 121852 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../01-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../02-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../03-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../04-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../05-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../07-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../08-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../09-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../10-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../11-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package nsight-systems-2023.3.3.\n",
            "Preparing to unpack .../12-nsight-systems-2023.3.3_2023.3.3.42-233333266658v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2023.3.3 (2023.3.3.42-233333266658v0) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up nsight-systems-2023.3.3 (2023.3.3.42-233333266658v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.3.3/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.3.3/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Found ncu in /usr/local/cuda/bin.\n",
            "\n",
            "=== Installation Check ===\n",
            "NVIDIA Nsight Systems version 2023.3.3.42-233333266658v0\n",
            "NVIDIA (R) Nsight Compute Command Line Profiler\n",
            "Copyright (c) 2018-2025 NVIDIA Corporation\n",
            "Version 2025.1.1.0 (build 35528883) (public-release)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 1. Nsight Systems (nsys) 설치 여부 확인 및 경로 설정\n",
        "if os.path.exists('/usr/local/cuda/bin/nsys'):\n",
        "    print(\"Found nsys in /usr/local/cuda/bin. Adding to PATH...\")\n",
        "    os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "else:\n",
        "    print(\"Installing Nsight Systems...\")\n",
        "    # 최신 버전 설치 (apt repository 업데이트)\n",
        "    !apt-get update -y\n",
        "    !apt-get install -y nsight-systems-2023.3.3\n",
        "    # 설치 후 경로 추가 (보통 /usr/local/bin에 생기지만 혹시 모르니)\n",
        "    os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "\n",
        "# 2. Nsight Compute (ncu) 설치 여부 확인\n",
        "if os.path.exists('/usr/local/cuda/bin/ncu'):\n",
        "    print(\"Found ncu in /usr/local/cuda/bin.\")\n",
        "else:\n",
        "    print(\"Installing Nsight Compute...\")\n",
        "    !apt-get install -y nsight-compute-2023.3.1\n",
        "    os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "\n",
        "print(\"\\n=== Installation Check ===\")\n",
        "!nsys --version\n",
        "!ncu --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ki4V9VrqNIK",
        "outputId": "3b048c65-6855-447c-cbc8-321e9aa3d3d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing profile_run.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile profile_run.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "import torch.cuda.nvtx as nvtx\n",
        "import time\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. CUDA Kernel Source (작성하신 코드 그대로)\n",
        "# ---------------------------------------------------------\n",
        "cuda_source = \"\"\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <torch/extension.h>\n",
        "\n",
        "template<int BM, int BN, int BK, int TM, int TN_PHY>\n",
        "__global__ void sgemm2D(float* Input, int* B, float* C, float* scale, float* zero_point,\n",
        "    int batch,\n",
        "    int In_H, int In_W, int In_C,\n",
        "    int Out_H, int Out_W,\n",
        "    int K_H, int K_W,\n",
        "    int Pad_H, int Pad_W,\n",
        "    int Stride_H, int Stride_W,\n",
        "    int Dilation_H, int Dilation_W)\n",
        "{\n",
        "    int cRow = blockIdx.y;\n",
        "    int cCol = blockIdx.x;\n",
        "\n",
        "    int M = batch * Out_H * Out_W;\n",
        "    int N = gridDim.x * BN;\n",
        "    int K = In_C * K_H * K_W;\n",
        "\n",
        "    constexpr int TN_LOG = TN_PHY * 8;\n",
        "\n",
        "    int threadRow = threadIdx.x / (BN / TN_LOG);\n",
        "    int threadCol = threadIdx.x % (BN / TN_LOG);\n",
        "\n",
        "    __shared__ float As[BM * BK];\n",
        "    __shared__ int Bs[BK * (BN / 8)];\n",
        "\n",
        "    B += cCol * (BN / 8);\n",
        "    C += cRow * BM * N + cCol * BN;\n",
        "\n",
        "    float threadResults[TM * TN_LOG] = {0.0};\n",
        "    float regM[TM] = {0.0};\n",
        "    int regN[TN_PHY] = {0};\n",
        "\n",
        "    // ★ 최적화: K 차원에 대해 입력 A의 값만 순수하게 누적해둘 배열\n",
        "    float sumA[TM] = {0.0f};\n",
        "\n",
        "    // (기존의 my_scales, my_zeros 로드 부분 완전히 삭제됨)\n",
        "\n",
        "    for (int bk = 0; bk < K; bk += BK) {\n",
        "\n",
        "        for (int idx = threadIdx.x; idx < BK * BM; idx += blockDim.x) {\n",
        "            int r = idx % BM;\n",
        "            int c = idx / BM;\n",
        "\n",
        "            int globalM = cRow * BM + r;\n",
        "            int curr_k = bk + c;\n",
        "\n",
        "            float val = 0.0f;\n",
        "            if (curr_k < K && globalM < M) {\n",
        "                int area_out = Out_H * Out_W;\n",
        "                int batch_idx = globalM / area_out;\n",
        "                int pixel_rem = globalM % area_out;\n",
        "                int oh = pixel_rem / Out_W;\n",
        "                int ow = pixel_rem % Out_W;\n",
        "\n",
        "                int kc = curr_k % K_W;\n",
        "                int k_rem = curr_k / K_W;\n",
        "                int kh = k_rem % K_H;\n",
        "                int ic = k_rem / K_H;\n",
        "\n",
        "                int ih = oh * Stride_H - Pad_H + kh * Dilation_H;\n",
        "                int iw = ow * Stride_W - Pad_W + kc * Dilation_W;\n",
        "\n",
        "                if (ih >= 0 && ih < In_H && iw >= 0 && iw < In_W) {\n",
        "                    long long input_idx =\n",
        "                        (long long)batch_idx * (In_C * In_H * In_W) +\n",
        "                        (long long)ic * (In_H * In_W) +\n",
        "                        (long long)ih * In_W + iw;\n",
        "                    val = Input[input_idx];\n",
        "                }\n",
        "            }\n",
        "            As[c * BM + r] = val;\n",
        "        }\n",
        "\n",
        "        int num_int4 = (BK * BN / 8) / 4;\n",
        "        for (int idx = threadIdx.x; idx < num_int4; idx += blockDim.x) {\n",
        "            int r = idx / ((BN / 8) / 4);\n",
        "            int c_int4 = idx % ((BN / 8) / 4);\n",
        "            int c = c_int4 * 4;\n",
        "\n",
        "            if (bk + r < K) {\n",
        "                reinterpret_cast<int4*>(&Bs[r * (BN / 8) + c])[0] =\n",
        "                    reinterpret_cast<int4*>(&B[r * (N / 8) + c])[0];\n",
        "            } else {\n",
        "                Bs[r * (BN / 8) + c + 0] = 0;\n",
        "                Bs[r * (BN / 8) + c + 1] = 0;\n",
        "                Bs[r * (BN / 8) + c + 2] = 0;\n",
        "                Bs[r * (BN / 8) + c + 3] = 0;\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        B += BK * (N / 8);\n",
        "\n",
        "        for (int dot = 0; dot < BK; ++dot) {\n",
        "            for (int i = 0; i < TM; ++i){\n",
        "                float a_val = As[dot * BM + threadRow * TM + i];\n",
        "                regM[i] = a_val;\n",
        "                sumA[i] += a_val; // ★ 핵심 최적화: A의 값을 여기서 단순 누적\n",
        "            }\n",
        "            for (int i = 0; i < TN_PHY; ++i) {\n",
        "                regN[i] = Bs[dot * (BN / 8) + threadCol * TN_PHY + i];\n",
        "            }\n",
        "\n",
        "            for (int i = 0; i < TN_PHY; ++i) {\n",
        "                int packed_val = regN[i];\n",
        "\n",
        "                for (int subN = 0; subN < 8; ++subN) {\n",
        "                    int int4_val = (packed_val >> (subN * 4)) & 0xF;\n",
        "                    float w_val = float(int4_val); // ★ 연산 축소: 4-bit 값을 바로 float로만 변환\n",
        "\n",
        "                    for (int m = 0; m < TM; ++m) {\n",
        "                        int resNidx = i * 8 + subN;\n",
        "                        // ★ 연산 축소: Z빼고 S곱하는 무거운 부동소수점 연산 제거\n",
        "                        threadResults[m * TN_LOG + resNidx] += regM[m] * w_val;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // ★ 커널이 종료되기 직전, 수학의 분배법칙을 활용해 단 1번만 스케일/제로포인트 적용\n",
        "    for (int resIdxN = 0; resIdxN < TN_LOG; ++resIdxN) {\n",
        "        int globalN = cCol * BN + threadCol * TN_LOG + resIdxN;\n",
        "        float s = 1.0f;\n",
        "        float z = 0.0f;\n",
        "\n",
        "        // 메모리에서 여기서 불러옴 (글로벌 메모리 병목 회피)\n",
        "        if (globalN < N) {\n",
        "            s = scale[globalN];\n",
        "            z = zero_point[globalN];\n",
        "        }\n",
        "\n",
        "        for (int resIdxM = 0; resIdxM < TM; ++resIdxM) {\n",
        "            // C = (A*W - sum(A)*Z) * S\n",
        "            threadResults[resIdxM * TN_LOG + resIdxN] =\n",
        "                (threadResults[resIdxM * TN_LOG + resIdxN] - sumA[resIdxM] * z) * s;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (uint resIdxM = 0; resIdxM < TM; resIdxM += 1) {\n",
        "        for (uint resIdxN = 0; resIdxN < TN_LOG; resIdxN += 4) {\n",
        "            int globalRowC = cRow * BM + threadRow * TM + resIdxM;\n",
        "            int globalColC = cCol * BN + threadCol * TN_LOG + resIdxN;\n",
        "\n",
        "            if (globalRowC < M) {\n",
        "                float4 tmp;\n",
        "                tmp.x = threadResults[resIdxM * TN_LOG + resIdxN];\n",
        "                tmp.y = threadResults[resIdxM * TN_LOG + resIdxN + 1];\n",
        "                tmp.z = threadResults[resIdxM * TN_LOG + resIdxN + 2];\n",
        "                tmp.w = threadResults[resIdxM * TN_LOG + resIdxN + 3];\n",
        "\n",
        "                if (globalColC + 4 <= N) {\n",
        "                    reinterpret_cast<float4 *>(&C[(threadRow * TM + resIdxM) * N + threadCol * TN_LOG + resIdxN])[0] = tmp;\n",
        "                }\n",
        "                else {\n",
        "                    float* ptr = &C[(threadRow * TM + resIdxM) * N + threadCol * TN_LOG + resIdxN];\n",
        "                    if (globalColC + 0 < N) ptr[0] = tmp.x;\n",
        "                    if (globalColC + 1 < N) ptr[1] = tmp.y;\n",
        "                    if (globalColC + 2 < N) ptr[2] = tmp.z;\n",
        "                    if (globalColC + 3 < N) ptr[3] = tmp.w;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void sgemm_int4_cuda(\n",
        "    torch::Tensor Input,\n",
        "    torch::Tensor B_packed,\n",
        "    torch::Tensor C,\n",
        "    torch::Tensor scale,\n",
        "    torch::Tensor zero_point,\n",
        "    int batch, int In_H, int In_W, int In_C,\n",
        "    int Out_H, int Out_W,\n",
        "    int K_H, int K_W,\n",
        "    int Pad_H, int Pad_W,\n",
        "    int Stride_H, int Stride_W,\n",
        "    int Dilation_H, int Dilation_W\n",
        ") {\n",
        "    int M = batch * Out_H * Out_W;\n",
        "    int K = In_C * K_H * K_W;\n",
        "    int N = C.size(1);\n",
        "\n",
        "    const int BM = 64;\n",
        "    const int BN = 64;\n",
        "    const int BK = 8;\n",
        "    const int TM = 4;\n",
        "    const int TN_PHY = 1;\n",
        "\n",
        "    TORCH_CHECK(N % BN == 0, \"N must be a multiple of 128\");\n",
        "    TORCH_CHECK(K % 4 == 0, \"K must be a multiple of 4 for float4 loading\");\n",
        "    TORCH_CHECK(N % 32 == 0, \"N must be a multiple of 32 for int4 loading\");\n",
        "\n",
        "    TORCH_CHECK(Input.is_contiguous(), \"Input must be contiguous\");\n",
        "    TORCH_CHECK(B_packed.is_contiguous(), \"B_packed must be contiguous\");\n",
        "    TORCH_CHECK(C.is_contiguous(), \"C must be contiguous\");\n",
        "    TORCH_CHECK(scale.is_contiguous(), \"scale must be contiguous\");\n",
        "    TORCH_CHECK(zero_point.is_contiguous(), \"zero_point must be contiguous\");\n",
        "\n",
        "    int num_threads = (BM * BN) / (TM * (TN_PHY * 8));\n",
        "    dim3 blockDim(num_threads);\n",
        "    dim3 gridDim((N + BN - 1) / BN, (M + BM - 1) / BM);\n",
        "\n",
        "    sgemm2D<BM, BN, BK, TM, TN_PHY><<<gridDim, blockDim>>>(\n",
        "        Input.data_ptr<float>(),\n",
        "        B_packed.data_ptr<int>(),\n",
        "        C.data_ptr<float>(),\n",
        "        scale.data_ptr<float>(),\n",
        "        zero_point.data_ptr<float>(),\n",
        "        batch, In_H, In_W, In_C,\n",
        "        Out_H, Out_W,\n",
        "        K_H, K_W,\n",
        "        Pad_H, Pad_W,\n",
        "        Stride_H, Stride_W,\n",
        "        Dilation_H, Dilation_W\n",
        "    );\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "cpp_source = \"\"\"\n",
        "void sgemm_int4_cuda(\n",
        "    torch::Tensor Input,\n",
        "    torch::Tensor B_packed,\n",
        "    torch::Tensor C,\n",
        "    torch::Tensor scale,\n",
        "    torch::Tensor zero_point,\n",
        "    int batch, int In_H, int In_W, int In_C,\n",
        "    int Out_H, int Out_W,\n",
        "    int K_H, int K_W,\n",
        "    int Pad_H, int Pad_W,\n",
        "    int Stride_H, int Stride_W,\n",
        "    int Dilation_H, int Dilation_W\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Compile (JIT)\n",
        "# ---------------------------------------------------------\n",
        "sgemm_module = load_inline(\n",
        "    name=\"sgemm_int4_v1\",\n",
        "    cpp_sources=cpp_source,\n",
        "    cuda_sources=cuda_source, # 주의: 실제로는 위에서 정의한 긴 문자열이 들어가야 합니다.\n",
        "    functions=['sgemm_int4_cuda'],\n",
        "    verbose=False,\n",
        "    with_cuda=True,\n",
        "    extra_cuda_cflags=[\"-O3\", \"-lineinfo\"]\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Quantized Layer Definition\n",
        "# ---------------------------------------------------------\n",
        "class QuantizedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True,\n",
        "                 BM=128, BK=8, BN=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation # Dilation 추가\n",
        "        self.kernel_size = kernel_size\n",
        "        self.BM = BM\n",
        "        self.BK = BK\n",
        "        self.BN = BN\n",
        "\n",
        "        self.K = self.in_channels * self.kernel_size * self.kernel_size\n",
        "        self.N = self.out_channels\n",
        "\n",
        "        self.pad_n = (BN - self.N % BN) % BN\n",
        "        self.pad_k = (BK - self.K % BK) % BK\n",
        "\n",
        "        self.register_buffer('w_packed', torch.zeros(self.K + self.pad_k, (self.N + self.pad_n) // 8, dtype=torch.int32))\n",
        "        self.register_buffer('scale', torch.ones(self.N + self.pad_n, dtype=torch.float32))\n",
        "        self.register_buffer('zero_point', torch.zeros(self.N + self.pad_n, dtype=torch.float32))\n",
        "\n",
        "        if bias:\n",
        "            self.register_buffer('bias', torch.zeros(self.N + self.pad_n, dtype=torch.float32))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. 입력 차원 추출\n",
        "        batch_size, in_channels, h_in, w_in = x.shape\n",
        "\n",
        "        # 2. 출력 차원 계산\n",
        "        h_out = (h_in + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n",
        "        w_out = (w_in + 2 * self.padding - self.dilation * (self.kernel_size - 1) - 1) // self.stride + 1\n",
        "\n",
        "        # GEMM 관점에서의 M 차원 (Total number of output pixels per channel)\n",
        "        M = batch_size * h_out * w_out\n",
        "        N_padded = self.w_packed.shape[1] * 8\n",
        "\n",
        "        assert N_padded % self.BN == 0, \"N축 패딩 필요\"\n",
        "\n",
        "        # 3. 출력 텐서 (C) 메모리 할당 (M x N)\n",
        "        C_padded = torch.empty((M, N_padded), dtype=torch.float32, device=x.device)\n",
        "\n",
        "        # 메모리가 연속적인지 확인 (NCHW 포맷 그대로 커널에 전달)\n",
        "        x = x.contiguous()\n",
        "\n",
        "        # 4. Implicit GEMM 커널 호출 (F.unfold 없이 원본 x 전달)\n",
        "        # ※ 주의: sgemm_module의 Pybind11 바인딩 함수도 아래 인자들을 받도록 수정되어 있어야 합니다.\n",
        "        sgemm_module.sgemm_int4_cuda(\n",
        "            x,               # Input (Batch, In_C, In_H, In_W)\n",
        "            self.w_packed,   # B (Packed Weights)\n",
        "            C_padded,        # C (Output Matrix)\n",
        "            self.scale,\n",
        "            self.zero_point,\n",
        "            batch_size,\n",
        "            h_in, w_in, in_channels,\n",
        "            h_out, w_out,\n",
        "            self.kernel_size, self.kernel_size,\n",
        "            self.padding, self.padding,\n",
        "            self.stride, self.stride,\n",
        "            self.dilation, self.dilation\n",
        "        )\n",
        "\n",
        "        return C_padded\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Main Execution Block for Profiling\n",
        "# ---------------------------------------------------------\n",
        "def main():\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    # 모델 생성 (ResNet 전체 대신 무거운 레이어 하나만 테스트해도 충분합니다)\n",
        "    # 실제로는 전체 모델을 로드해서 하셔도 됩니다.\n",
        "    model = QuantizedConv2d(64, 128, kernel_size=3, stride=1, padding=1).to(device)\n",
        "    dummy_input = torch.randn(32, 64, 32, 32).to(device)\n",
        "\n",
        "    # Warm-up\n",
        "    print(\"Warm-up...\")\n",
        "    for _ in range(5):\n",
        "        model(dummy_input)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Profiling Run\n",
        "    print(\"Profiling Start...\")\n",
        "\n",
        "    # NVTX Range로 전체 구간 표시\n",
        "    nvtx.range_push(\"My_Model_Inference\")\n",
        "\n",
        "    for i in range(10): # 10번 반복\n",
        "        nvtx.range_push(f\"Iter_{i}\")\n",
        "        model(dummy_input)\n",
        "        torch.cuda.synchronize() # 정확한 시간 측정을 위해 (배포시는 제거)\n",
        "        nvtx.range_pop()\n",
        "\n",
        "    nvtx.range_pop()\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uNzNAWgAhdz",
        "outputId": "da382e02-97cb-469a-8665-6ffe156e43c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warm-up...\n",
            "Profiling Start...\n",
            "Done.\n",
            "Generating '/tmp/nsys-report-a59d.qdstrm'\n",
            "[1/7] [========================100%] nsys_result_32_im2col_fuse.nsys-rep\n",
            "[2/7] [========================100%] nsys_result_32_im2col_fuse.sqlite\n",
            "[3/7] Executing 'nvtx_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)   Style         Range       \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  -------  ------------------\n",
            "     50.0       51,499,099          1  51,499,099.0  51,499,099.0  51,499,099  51,499,099          0.0  PushPop  My_Model_Inference\n",
            "      5.1        5,204,104          1   5,204,104.0   5,204,104.0   5,204,104   5,204,104          0.0  PushPop  Iter_0            \n",
            "      5.0        5,159,819          1   5,159,819.0   5,159,819.0   5,159,819   5,159,819          0.0  PushPop  Iter_1            \n",
            "      5.0        5,146,267          1   5,146,267.0   5,146,267.0   5,146,267   5,146,267          0.0  PushPop  Iter_5            \n",
            "      5.0        5,142,368          1   5,142,368.0   5,142,368.0   5,142,368   5,142,368          0.0  PushPop  Iter_4            \n",
            "      5.0        5,141,772          1   5,141,772.0   5,141,772.0   5,141,772   5,141,772          0.0  PushPop  Iter_2            \n",
            "      5.0        5,141,021          1   5,141,021.0   5,141,021.0   5,141,021   5,141,021          0.0  PushPop  Iter_9            \n",
            "      5.0        5,139,630          1   5,139,630.0   5,139,630.0   5,139,630   5,139,630          0.0  PushPop  Iter_3            \n",
            "      5.0        5,123,450          1   5,123,450.0   5,123,450.0   5,123,450   5,123,450          0.0  PushPop  Iter_8            \n",
            "      5.0        5,116,329          1   5,116,329.0   5,116,329.0   5,116,329   5,116,329          0.0  PushPop  Iter_6            \n",
            "      5.0        5,114,152          1   5,114,152.0   5,114,152.0   5,114,152   5,114,152          0.0  PushPop  Iter_7            \n",
            "\n",
            "[4/7] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)                Name               \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ----------  -----------  ---------------------------------\n",
            "     89.6       75,241,292         11  6,840,117.5  5,032,341.0  4,994,242  24,964,946  6,011,338.3  cudaDeviceSynchronize            \n",
            "      3.0        2,529,005          1  2,529,005.0  2,529,005.0  2,529,005   2,529,005          0.0  cudaGetDeviceProperties_v2_v12000\n",
            "      2.6        2,156,588         15    143,772.5     14,974.0      7,206   1,936,786    496,063.9  cudaLaunchKernel                 \n",
            "      2.3        1,923,403          5    384,680.6     17,085.0      5,257   1,842,445    815,110.5  cudaMemcpyAsync                  \n",
            "      1.5        1,281,389          3    427,129.7      5,320.0      2,626   1,273,443    732,930.1  cudaStreamIsCapturing_v10000     \n",
            "      0.9          730,647          3    243,549.0    243,700.0    203,854     283,093     39,619.7  cudaMalloc                       \n",
            "      0.1          122,279          5     24,455.8      9,138.0      6,001      83,984     33,479.6  cudaStreamSynchronize            \n",
            "      0.0            1,569          1      1,569.0      1,569.0      1,569       1,569          0.0  cuModuleGetLoadingMode           \n",
            "\n",
            "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                                                  Name                                                \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------------------------------------------------------------\n",
            "    100.0       75,860,592         15  5,057,372.8  5,057,665.0  5,054,945  5,060,321      1,484.9  void sgemm2D<(int)64, (int)64, (int)8, (int)4, (int)1>(float *, int *, float *, float *, float *, i…\n",
            "\n",
            "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)  Min (ns)  Max (ns)   StdDev (ns)           Operation          \n",
            " --------  ---------------  -----  ---------  --------  --------  ---------  -----------  ----------------------------\n",
            "    100.0        1,684,885      5  336,977.0     704.0       704  1,677,365    749,302.4  [CUDA memcpy Host-to-Device]\n",
            "\n",
            "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
            "      8.427      5     1.685     0.001     0.001     8.389        3.747  [CUDA memcpy Host-to-Device]\n",
            "\n",
            "Generated:\n",
            "    /content/nsys_result_32_im2col_fuse.nsys-rep\n",
            "    /content/nsys_result_32_im2col_fuse.sqlite\n"
          ]
        }
      ],
      "source": [
        "# --trace=cuda,nvtx,osrt  <-- 여기서 osrt 제거\n",
        "!nsys profile \\\n",
        "  --trace=cuda,nvtx \\\n",
        "  --output=nsys_result_32_im2col_fuse \\\n",
        "  --force-overwrite=true \\\n",
        "  --stats=true \\\n",
        "  python profile_run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdtkqFeqApsQ",
        "outputId": "fcbf2131-874b-499c-a25f-153c7636b8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: ./sgemm_run: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!./sgemm_run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTexcp6GYKCn",
        "outputId": "a45d4f22-1a82-4ea8-e761-9dec14ba9932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==PROF== Connected to process 2318 (/usr/bin/python3.12)\n",
            "Warm-up...\n",
            "Profiling Start...\n",
            "Done.\n",
            "==PROF== Disconnected from process 2318\n",
            "==WARNING== No kernels were profiled.\n",
            "Available Kernels:\n",
            "1. sgemm2D\n"
          ]
        }
      ],
      "source": [
        "# Colab 셀에서 실행\n",
        "!ncu \\\n",
        "  --set full \\\n",
        "  --kernel-name regex:sgemm2D \\\n",
        "  --launch-count 1 \\\n",
        "  -o ncu_result_32_im2col_fuse \\\n",
        "  -f \\\n",
        "  python profile_run.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
